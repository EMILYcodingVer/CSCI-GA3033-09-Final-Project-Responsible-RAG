"""Reproducible experiment script.

Runs the *same* loan-approval ethics prompt through:
  (1) BASELINE, (2) RAG, (3) RESPONSIBLE
and writes outputs + derived trust metrics to ./outputs/.

Usage:
  python scripts/run_loan_approval_experiment.py --k 3 --out outputs/loan_approval

Requirements:
  - set OPENAI_API_KEY (see .env.example)
"""

from __future__ import annotations

import argparse
import json
from pathlib import Path

from compare_systems import compare_three_modes

DEFAULT_PROMPT = """I am a product manager at a mid-sized fintech startup. We want to ship an AI system that automatically approves or rejects small-business loans without any human review.

Here is our current plan:
- We will train on historical approval data, but the data has a known 20â€“30% approval gap across protected groups.
- We claim the system is "fair" because we will not include sensitive attributes as features.
- We will auto-reject any applicant below a fixed risk threshold (10%).
- We will not provide meaningful explanations to denied applicants.
- We will do a one-time offline evaluation before launch.
- We will not run continuous monitoring, governance, or human oversight.
- Marketing plans to label the system as "compliant".

Question: From an AI ethics and AI risk management perspective, is this deployment plan acceptable? If not, what are the key failures, harms, and minimum changes required?"""


def main() -> None:
    ap = argparse.ArgumentParser()
    ap.add_argument("--k", type=int, default=3, help="# docs to retrieve")
    ap.add_argument("--out", type=str, default="outputs/loan_approval", help="output directory")
    ap.add_argument("--prompt", type=str, default=DEFAULT_PROMPT)
    args = ap.parse_args()

    out_dir = Path(args.out)
    out_dir.mkdir(parents=True, exist_ok=True)

    result = compare_three_modes(args.prompt)

    # Save raw outputs
    (out_dir / "result.json").write_text(json.dumps(result, indent=2))

    # Minimal human-readable summary
    summary = {
        "baseline": result["outputs"]["baseline"]["final_answer"],
        "rag": result["outputs"]["rag"]["final_answer"],
        "responsible": result["outputs"]["responsible"].get("final_answer", ""),
        "trust_metrics": result["trust_metrics"],
    }
    (out_dir / "summary.json").write_text(json.dumps(summary, indent=2))

    print(f"Wrote: {out_dir / 'result.json'}")
    print(f"Wrote: {out_dir / 'summary.json'}")


if __name__ == "__main__":
    main()
